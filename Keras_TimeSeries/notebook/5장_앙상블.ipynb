{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate\n",
    "2개 이상의 모델을 합치는 방법을 배울 것임.  \n",
    "항상 1개의 모델만을 쓴다면 편하겠지만, 데이터 분석쪽 일을 하다보면 앞으로 많은 데이터를 만나게 되고, 순차 모델 하나만으로는 해결할 수 없음.  \n",
    "이전까지 우리는 컬럼이 여러개인 문제등을 해결해 봄.  \n",
    "앙상블 장에서는 컬럼 뿐만 아니라 모델도 여러개인 문제를 다룸  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 준비. x값으로 y 값이 유추되지 않도록 데이터 컬럼을 섞음  \n",
    "각각 300개의 데이터를 가지고 있는 2개의 x값과 100개의 데이터를 가지고 있는 1개의 y값을 준비함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비\n",
    "x1 = np.array([range(100), range(311, 411), range(100)])\n",
    "x2 = np.array([range(101, 201), range(311, 411), range(101, 201)])\n",
    "\n",
    "y = np.array([range(501, 601)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터는 3개의 컬럼을 가지고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1, x2 shape :  (100, 3) (100, 3) \n",
      "y shape:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.transpose(x1)\n",
    "x2 = np.transpose(x2)\n",
    "\n",
    "y = np.transpose(y)\n",
    "\n",
    "print('x1, x2 shape : ', x1.shape, x2.shape,'\\ny shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, val, test 로 데이터 분리해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x1_train, x1_test, y_train, t_test = train_test_split(x1, y, test_size=0.4, random_state=66, shuffle=False)\n",
    "x1_val, x1_test, y_val, y_test = train_test_split(x1_test, t_test, test_size=0.5, random_state=66, shuffle=False)\n",
    "\n",
    "x2_train, x2_test = train_test_split(x2, test_size=0.4, random_state=66, shuffle=False)\n",
    "x2_val, x2_test = train_test_split(x2_test, test_size=0.5, random_state=66, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape(train, val, test) :  (60, 3) (20, 3) (20, 3)\n",
      "x2 shape(train, val, test) :  (60, 3) (20, 3) (20, 3)\n",
      "y shape(train, val, test) :  (60, 1) (20, 1) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x1 shape(train, val, test) : \", x1_train.shape, x1_val.shape, x1_test.shape)\n",
    "print(\"x2 shape(train, val, test) : \", x2_train.shape, x2_val.shape, x2_test.shape)\n",
    "print(\"y shape(train, val, test) : \", y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 구성함. 2개의 모델을 만든 뒤 2개의 모델을 병합하는 방식 = 앙상블 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 구성\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# 모델 1\n",
    "input1 = Input(shape=(3,))\n",
    "dense1 = Dense(100, activation='relu')(input1)\n",
    "dense1_2 = Dense(30)(dense1)\n",
    "dense1_3 = Dense(7)(dense1_2)\n",
    "\n",
    "# 모델 2\n",
    "input2 = Input(shape=(3,))\n",
    "dense2 = Dense(50, activation='relu')(input2)\n",
    "dense2_1 = Dense(7)(dense2)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간의 히든 레이어의 깊이는 똑같을 필요가 없음. 이를 도식화 하면 다음과 같음\n",
    "![image.png](attachment:image.png)\n",
    "merge 하는 방법은 간단함. `keras.layers.merge` or `keras.layers` 에서 `concatenate` 를 사용하면 됨  \n",
    "합치려는 모델의 마지막레이어들 리스트 형식으로 전달해주면 됨. `concatenate([model 1의 마지막 레이어, model 2의 마지막 레이어]`  \n",
    "Concatenate() 도 존재 ()붙여 사용하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "merge1 = concatenate([dense1_3, dense2_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병합된 이후에는 앞에서 했던 것처럼 함수형으로 모델을 이어나가면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Dense(10)(merge1)\n",
    "model2 = Dense(5)(model1)\n",
    "output = Dense(1)(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output 레이어까지 준비가 되었으니, `Model()`을 사용하여 모델들을 정의해봄. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 100)          400         input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 30)           3030        dense_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 50)           200         input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 7)            217         dense_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 7)            357         dense_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 14)           0           dense_69[0][0]                   \n",
      "                                                                 dense_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 10)           150         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 5)            55          dense_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 1)            6           dense_73[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,415\n",
      "Trainable params: 4,415\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = [input1, input2], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다섯번째 dense와 여섯번째 dense 사이에서 concatenate 가 잘 이루어진 것을 확인  \n",
    "훈련을 진행함.  \n",
    "훈련을 할때 `x`의 입력데이터가 2개 이므로 list 형태를 취하여 들어가야함. `validation_data` 도 마찬가지로 2개의 데이터이므로 리스트 형태로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 108147.2577 - mse: 108147.2577 - val_loss: 70.0662 - val_mse: 70.0662\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 245.9274 - mse: 245.9274 - val_loss: 344.8391 - val_mse: 344.8391\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 62.1775 - mse: 62.1775 - val_loss: 105.4446 - val_mse: 105.4446\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 21.1234 - mse: 21.1234 - val_loss: 1.2421 - val_mse: 1.2421\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 8.7383 - mse: 8.7383 - val_loss: 1.3765 - val_mse: 1.3765\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.6959 - mse: 0.6959 - val_loss: 1.0329 - val_mse: 1.0329\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.5291 - mse: 0.5291 - val_loss: 1.3514 - val_mse: 1.3514\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4755 - mse: 0.4755 - val_loss: 1.8932 - val_mse: 1.8932\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4254 - mse: 0.4254 - val_loss: 0.4142 - val_mse: 0.4142\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.7490 - mse: 0.7490 - val_loss: 3.3351 - val_mse: 3.3351\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.4101 - mse: 0.4101 - val_loss: 0.6755 - val_mse: 0.6755\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3334 - mse: 0.3334 - val_loss: 1.8160 - val_mse: 1.8160\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3401 - mse: 0.3401 - val_loss: 2.2830 - val_mse: 2.2830\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3474 - mse: 0.3474 - val_loss: 0.8756 - val_mse: 0.8756\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2734 - mse: 0.2734 - val_loss: 0.4837 - val_mse: 0.4837\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3181 - mse: 0.3181 - val_loss: 0.4998 - val_mse: 0.4998\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2677 - mse: 0.2677 - val_loss: 2.8982 - val_mse: 2.8982\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3873 - mse: 0.3873 - val_loss: 0.8489 - val_mse: 0.8489\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2973 - mse: 0.2973 - val_loss: 1.9968 - val_mse: 1.9968\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2290 - mse: 0.2290 - val_loss: 0.5141 - val_mse: 0.5141\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1654 - mse: 0.1654 - val_loss: 0.0523 - val_mse: 0.0523\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 1.4241 - val_mse: 1.4241\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3044 - mse: 0.3044 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2355 - mse: 0.2355 - val_loss: 0.2194 - val_mse: 0.2194\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.0603 - val_mse: 0.0603\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2049 - mse: 0.2049 - val_loss: 0.0817 - val_mse: 0.0817\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.7119 - val_mse: 0.7119\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3349 - mse: 0.3349 - val_loss: 1.2774 - val_mse: 1.2774\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1488 - mse: 0.1488 - val_loss: 0.3825 - val_mse: 0.3825\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1715 - mse: 0.1715 - val_loss: 0.5913 - val_mse: 0.5913\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1933 - mse: 0.1933 - val_loss: 0.7446 - val_mse: 0.7446\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2076 - mse: 0.2076 - val_loss: 0.1818 - val_mse: 0.1818\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2578 - mse: 0.2578 - val_loss: 0.8815 - val_mse: 0.8815\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1248 - mse: 0.1248 - val_loss: 0.4417 - val_mse: 0.4417\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.2589 - mse: 0.2589 - val_loss: 0.4997 - val_mse: 0.4997\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1728 - mse: 0.1728 - val_loss: 2.6441 - val_mse: 2.6441\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.4162 - mse: 0.4162 - val_loss: 0.6267 - val_mse: 0.6267\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1226 - mse: 0.1226 - val_loss: 0.1698 - val_mse: 0.1698\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1601 - mse: 0.1601 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.5952 - val_mse: 0.5952\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1803 - mse: 0.1803 - val_loss: 0.5399 - val_mse: 0.5399\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.0533 - val_mse: 0.0533\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0522 - mse: 0.0522 - val_loss: 0.0635 - val_mse: 0.0635\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0671 - mse: 0.0671 - val_loss: 0.2589 - val_mse: 0.2589\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1188 - mse: 0.1188 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0627 - mse: 0.0627 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0427 - val_mse: 0.0427\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0473 - mse: 0.0473 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 53/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 54/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 55/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 56/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0501 - val_mse: 0.0501\n",
      "Epoch 57/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 4.4760e-04 - val_mse: 4.4760e-04\n",
      "Epoch 58/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 59/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 60/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0494 - mse: 0.0494 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 61/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0927 - mse: 0.0927 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 62/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 1.5118e-04 - val_mse: 1.5118e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 64/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 65/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 66/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 67/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0709 - val_mse: 0.0709\n",
      "Epoch 68/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0452 - val_mse: 0.0452\n",
      "Epoch 69/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 70/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 71/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 72/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.1125 - val_mse: 0.1125\n",
      "Epoch 73/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 74/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.7069e-04 - val_mse: 1.7069e-04\n",
      "Epoch 75/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 4.1144e-04 - val_mse: 4.1144e-04\n",
      "Epoch 76/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 6.2232e-04 - mse: 6.2232e-04 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 77/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0519 - mse: 0.0519 - val_loss: 0.1342 - val_mse: 0.1342\n",
      "Epoch 78/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.5018 - mse: 0.5018 - val_loss: 0.4810 - val_mse: 0.4810\n",
      "Epoch 79/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 6.7882e-04 - val_mse: 6.7882e-04\n",
      "Epoch 80/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 81/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.7948e-04 - mse: 1.7948e-04 - val_loss: 2.1705e-04 - val_mse: 2.1705e-04\n",
      "Epoch 82/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 4.8577e-04 - val_mse: 4.8577e-04\n",
      "Epoch 83/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 84/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.2445 - val_mse: 0.2445\n",
      "Epoch 85/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 86/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0829 - mse: 0.0829 - val_loss: 0.2114 - val_mse: 0.2114\n",
      "Epoch 87/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 1.0290e-04 - val_mse: 1.0290e-04\n",
      "Epoch 88/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 89/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2461 - mse: 0.2461 - val_loss: 18.1106 - val_mse: 18.1106\n",
      "Epoch 90/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 22.6774 - mse: 22.6774 - val_loss: 7.9261 - val_mse: 7.9261\n",
      "Epoch 91/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.4648 - mse: 1.4648 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 92/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2914 - mse: 0.2914 - val_loss: 0.1446 - val_mse: 0.1446\n",
      "Epoch 93/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1719 - mse: 0.1719 - val_loss: 3.2303 - val_mse: 3.2303\n",
      "Epoch 94/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.7720 - mse: 0.7720 - val_loss: 0.0937 - val_mse: 0.0937\n",
      "Epoch 95/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1010 - mse: 0.1010 - val_loss: 0.2280 - val_mse: 0.2280\n",
      "Epoch 96/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1895 - mse: 0.1895 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 97/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1106 - mse: 0.1106 - val_loss: 0.0571 - val_mse: 0.0571\n",
      "Epoch 98/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 99/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.5511 - val_mse: 0.5511\n",
      "Epoch 100/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 13.0298 - mse: 13.0298 - val_loss: 8.5962 - val_mse: 8.5962\n",
      "Epoch 101/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 26.3736 - mse: 26.3736 - val_loss: 77.3663 - val_mse: 77.3663\n",
      "Epoch 102/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 27.9561 - mse: 27.9561 - val_loss: 22.9355 - val_mse: 22.9355\n",
      "Epoch 103/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 4.8985 - mse: 4.8985 - val_loss: 0.4648 - val_mse: 0.4648\n",
      "Epoch 104/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3069 - mse: 0.3069 - val_loss: 0.0478 - val_mse: 0.0478\n",
      "Epoch 105/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1235 - mse: 0.1235 - val_loss: 0.4644 - val_mse: 0.4644\n",
      "Epoch 106/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3943 - mse: 0.3943 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 107/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3276 - mse: 0.3276 - val_loss: 0.4591 - val_mse: 0.4591\n",
      "Epoch 108/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0942 - mse: 0.0942 - val_loss: 0.1465 - val_mse: 0.1465\n",
      "Epoch 109/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0751 - mse: 0.0751 - val_loss: 0.2557 - val_mse: 0.2557\n",
      "Epoch 110/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.5010 - mse: 0.5010 - val_loss: 1.9584 - val_mse: 1.9584\n",
      "Epoch 111/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.2240 - mse: 1.2240 - val_loss: 0.1090 - val_mse: 0.1090\n",
      "Epoch 112/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1134 - mse: 0.1134 - val_loss: 0.7637 - val_mse: 0.7637\n",
      "Epoch 113/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2507 - mse: 0.2507 - val_loss: 0.2277 - val_mse: 0.2277\n",
      "Epoch 114/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1195 - mse: 0.1195 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 115/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1176 - mse: 0.1176 - val_loss: 0.4022 - val_mse: 0.4022\n",
      "Epoch 116/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.3244 - val_mse: 0.3244\n",
      "Epoch 117/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2751 - mse: 0.2751 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 118/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1589 - mse: 0.1589 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 119/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0454 - mse: 0.0454 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 120/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0217 - val_mse: 0.0217\n",
      "Epoch 121/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 122/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 4.5005e-04 - val_mse: 4.5005e-04\n",
      "Epoch 123/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 124/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2062 - mse: 0.2062 - val_loss: 0.4534 - val_mse: 0.4534\n",
      "Epoch 125/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1981 - mse: 0.1981 - val_loss: 0.4368 - val_mse: 0.4368\n",
      "Epoch 126/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 35.2753 - val_mse: 35.2753\n",
      "Epoch 127/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.9583 - mse: 4.9583 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 128/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3551 - mse: 0.3551 - val_loss: 1.5447 - val_mse: 1.5447\n",
      "Epoch 129/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.7699 - mse: 0.7699 - val_loss: 0.1851 - val_mse: 0.1851\n",
      "Epoch 130/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2164 - mse: 0.2164 - val_loss: 0.3074 - val_mse: 0.3074\n",
      "Epoch 131/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2280 - mse: 0.2280 - val_loss: 0.2102 - val_mse: 0.2102\n",
      "Epoch 132/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.5911 - mse: 2.5911 - val_loss: 37.5295 - val_mse: 37.5295\n",
      "Epoch 133/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 13.7217 - mse: 13.7217 - val_loss: 0.0538 - val_mse: 0.0538\n",
      "Epoch 134/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.1253 - mse: 1.1253 - val_loss: 0.7361 - val_mse: 0.7361\n",
      "Epoch 135/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.4679 - mse: 0.4679 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 136/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.4473 - mse: 0.4473 - val_loss: 0.2739 - val_mse: 0.2739\n",
      "Epoch 137/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.4390 - mse: 0.4390 - val_loss: 0.1784 - val_mse: 0.1784\n",
      "Epoch 138/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0763 - mse: 0.0763 - val_loss: 5.9210 - val_mse: 5.9210\n",
      "Epoch 139/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.3324 - mse: 2.3324 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 140/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2277 - mse: 0.2277 - val_loss: 2.8949 - val_mse: 2.8949\n",
      "Epoch 141/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.9337 - mse: 1.9337 - val_loss: 0.4021 - val_mse: 0.4021\n",
      "Epoch 142/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2590 - mse: 0.2590 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 143/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.6931 - mse: 0.6931 - val_loss: 0.0602 - val_mse: 0.0602\n",
      "Epoch 144/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 66.2369 - mse: 66.2369 - val_loss: 75.5581 - val_mse: 75.5581\n",
      "Epoch 145/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 45.4396 - mse: 45.4396 - val_loss: 1.3084 - val_mse: 1.3084\n",
      "Epoch 146/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.1129 - mse: 9.1129 - val_loss: 5.3101 - val_mse: 5.3101\n",
      "Epoch 147/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.5245 - mse: 0.5245 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 148/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 4.1006e-04 - val_mse: 4.1006e-04\n",
      "Epoch 149/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 150/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 3.0410e-04 - mse: 3.0410e-04 - val_loss: 1.3467e-04 - val_mse: 1.3467e-04\n",
      "Epoch 151/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 152/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5.3578e-04 - mse: 5.3578e-04 - val_loss: 2.3611e-04 - val_mse: 2.3611e-04\n",
      "Epoch 153/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.5188e-04 - mse: 1.5188e-04 - val_loss: 3.7287e-04 - val_mse: 3.7287e-04\n",
      "Epoch 154/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.0176e-05 - mse: 9.0176e-05 - val_loss: 1.6419e-04 - val_mse: 1.6419e-04\n",
      "Epoch 155/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.4045e-05 - mse: 9.4045e-05 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 156/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 157/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0725 - mse: 0.0725 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 158/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 3.5222e-04 - mse: 3.5222e-04 - val_loss: 2.4496e-04 - val_mse: 2.4496e-04\n",
      "Epoch 159/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 8.3090e-05 - mse: 8.3090e-05 - val_loss: 3.0416e-04 - val_mse: 3.0416e-04\n",
      "Epoch 160/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.9685e-04 - mse: 1.9685e-04 - val_loss: 2.1956e-04 - val_mse: 2.1956e-04\n",
      "Epoch 161/300\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 2.7491e-05 - mse: 2.7491e-05 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 162/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.3842e-04 - mse: 2.3842e-04 - val_loss: 2.0420e-04 - val_mse: 2.0420e-04\n",
      "Epoch 163/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.1285e-04 - mse: 1.1285e-04 - val_loss: 3.5385e-04 - val_mse: 3.5385e-04\n",
      "Epoch 164/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 3.8360e-05 - mse: 3.8360e-05 - val_loss: 2.2772e-04 - val_mse: 2.2772e-04\n",
      "Epoch 165/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.7592e-05 - mse: 4.7592e-05 - val_loss: 2.9851e-04 - val_mse: 2.9851e-04\n",
      "Epoch 166/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.1095e-04 - mse: 1.1095e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 167/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 168/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.8999e-04 - val_mse: 5.8999e-04\n",
      "Epoch 169/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.2195e-04 - mse: 2.2195e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 170/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.9708e-04 - mse: 4.9708e-04 - val_loss: 3.9495e-04 - val_mse: 3.9495e-04\n",
      "Epoch 171/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 3.1363e-04 - mse: 3.1363e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 172/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.0673e-04 - mse: 2.0673e-04 - val_loss: 9.5879e-04 - val_mse: 9.5879e-04\n",
      "Epoch 173/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 8.8596e-05 - mse: 8.8596e-05 - val_loss: 5.1332e-04 - val_mse: 5.1332e-04\n",
      "Epoch 174/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.5900e-04 - mse: 4.5900e-04 - val_loss: 3.9850e-04 - val_mse: 3.9850e-04\n",
      "Epoch 175/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 8.0339e-05 - val_mse: 8.0339e-05\n",
      "Epoch 176/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 177/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1167 - val_mse: 0.1167\n",
      "Epoch 178/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.7851 - mse: 0.7851 - val_loss: 25.6043 - val_mse: 25.6043\n",
      "Epoch 179/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 256.7793 - mse: 256.7793 - val_loss: 26.8673 - val_mse: 26.8673\n",
      "Epoch 180/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.0587 - mse: 2.0587 - val_loss: 0.0422 - val_mse: 0.0422\n",
      "Epoch 181/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3350 - mse: 0.3350 - val_loss: 2.5766 - val_mse: 2.5766\n",
      "Epoch 182/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.6022 - mse: 0.6022 - val_loss: 1.7883 - val_mse: 1.7883\n",
      "Epoch 183/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2793 - mse: 0.2793 - val_loss: 0.1535 - val_mse: 0.1535\n",
      "Epoch 184/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1741 - mse: 0.1741 - val_loss: 0.8969 - val_mse: 0.8969\n",
      "Epoch 185/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2803 - mse: 0.2803 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 186/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1195 - mse: 0.1195 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 187/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0560 - mse: 0.0560 - val_loss: 0.1277 - val_mse: 0.1277\n",
      "Epoch 188/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0608 - mse: 0.0608 - val_loss: 0.1855 - val_mse: 0.1855\n",
      "Epoch 189/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1376 - mse: 0.1376 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 190/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 8.0776e-04 - val_mse: 8.0776e-04\n",
      "Epoch 191/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0693 - mse: 0.0693 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 192/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 193/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0563 - mse: 0.0563 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 194/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0480 - val_mse: 0.0480\n",
      "Epoch 195/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 196/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 1.8434e-04 - val_mse: 1.8434e-04\n",
      "Epoch 197/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 198/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 199/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 200/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 8.7073e-05 - val_mse: 8.7073e-05\n",
      "Epoch 201/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0474 - val_mse: 0.0474\n",
      "Epoch 202/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 203/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.2459 - val_mse: 0.2459\n",
      "Epoch 204/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0488 - mse: 0.0488 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 205/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 206/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 207/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.1523 - val_mse: 0.1523\n",
      "Epoch 208/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 209/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.1276 - val_mse: 0.1276\n",
      "Epoch 210/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1075 - mse: 0.1075 - val_loss: 0.0461 - val_mse: 0.0461\n",
      "Epoch 211/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0736 - mse: 0.0736 - val_loss: 2.1693e-04 - val_mse: 2.1693e-04\n",
      "Epoch 212/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3939 - mse: 0.3939 - val_loss: 0.0953 - val_mse: 0.0953\n",
      "Epoch 213/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1335 - mse: 0.1335 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 214/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 215/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 216/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 217/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 1.9877e-04 - val_mse: 1.9877e-04\n",
      "Epoch 218/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.1668 - val_mse: 0.1668\n",
      "Epoch 219/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1357 - mse: 0.1357 - val_loss: 4.8561 - val_mse: 4.8561\n",
      "Epoch 220/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.8918 - mse: 1.8918 - val_loss: 0.4984 - val_mse: 0.4984\n",
      "Epoch 221/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.5926 - mse: 1.5926 - val_loss: 4.2137 - val_mse: 4.2137\n",
      "Epoch 222/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5.5497 - mse: 5.5497 - val_loss: 4.6358 - val_mse: 4.6358\n",
      "Epoch 223/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.0763 - mse: 4.0763 - val_loss: 2.4381 - val_mse: 2.4381\n",
      "Epoch 224/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.6207 - mse: 2.6207 - val_loss: 0.1363 - val_mse: 0.1363\n",
      "Epoch 225/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 6.9377e-04 - val_mse: 6.9377e-04\n",
      "Epoch 226/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 227/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 2.0064e-04 - val_mse: 2.0064e-04\n",
      "Epoch 228/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 3.8004e-04 - mse: 3.8004e-04 - val_loss: 2.7259e-04 - val_mse: 2.7259e-04\n",
      "Epoch 229/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5.2452e-05 - mse: 5.2452e-05 - val_loss: 4.4305e-04 - val_mse: 4.4305e-04\n",
      "Epoch 230/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0979 - val_mse: 0.0979\n",
      "Epoch 231/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1337 - mse: 0.1337 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 232/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 9.8501e-04 - val_mse: 9.8501e-04\n",
      "Epoch 233/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 234/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2371 - mse: 0.2371 - val_loss: 1.7723 - val_mse: 1.7723\n",
      "Epoch 235/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.1907 - mse: 4.1907 - val_loss: 41.0505 - val_mse: 41.0505\n",
      "Epoch 236/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 20.6184 - mse: 20.6184 - val_loss: 7.0462 - val_mse: 7.0462\n",
      "Epoch 237/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.5571 - mse: 2.5571 - val_loss: 0.7696 - val_mse: 0.7696\n",
      "Epoch 238/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.7344 - mse: 2.7344 - val_loss: 15.4232 - val_mse: 15.4232\n",
      "Epoch 239/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5.5789 - mse: 5.5789 - val_loss: 4.6001 - val_mse: 4.6001\n",
      "Epoch 240/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.4289 - mse: 1.4289 - val_loss: 0.0420 - val_mse: 0.0420\n",
      "Epoch 241/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.1242 - val_mse: 0.1242\n",
      "Epoch 242/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2577 - mse: 0.2577 - val_loss: 1.4661 - val_mse: 1.4661\n",
      "Epoch 243/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2723 - mse: 0.2723 - val_loss: 0.2753 - val_mse: 0.2753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2750 - mse: 0.2750 - val_loss: 0.1567 - val_mse: 0.1567\n",
      "Epoch 245/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2588 - mse: 0.2588 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 246/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2927 - mse: 0.2927 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 247/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0699 - mse: 0.0699 - val_loss: 0.1755 - val_mse: 0.1755\n",
      "Epoch 248/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2753 - mse: 0.2753 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 249/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.1981 - mse: 0.1981 - val_loss: 1.2100 - val_mse: 1.2100\n",
      "Epoch 250/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2938 - mse: 0.2938 - val_loss: 0.0823 - val_mse: 0.0823\n",
      "Epoch 251/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 1.2871 - val_mse: 1.2871\n",
      "Epoch 252/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.6738 - mse: 0.6738 - val_loss: 2.8493 - val_mse: 2.8493\n",
      "Epoch 253/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.2626 - mse: 2.2626 - val_loss: 4.3168 - val_mse: 4.3168\n",
      "Epoch 254/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 19.2853 - mse: 19.2853 - val_loss: 31.6850 - val_mse: 31.6850\n",
      "Epoch 255/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 35.5112 - mse: 35.5112 - val_loss: 0.1038 - val_mse: 0.1038\n",
      "Epoch 256/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.2419 - mse: 0.2419 - val_loss: 0.1808 - val_mse: 0.1808\n",
      "Epoch 257/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0788 - mse: 0.0788 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 258/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.7224e-04 - mse: 9.7224e-04 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 259/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.5526e-04 - mse: 1.5526e-04 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 260/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 261/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.4866e-04 - mse: 9.4866e-04 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 262/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 263/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.5106e-04 - mse: 9.5106e-04 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 264/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.6062e-04 - mse: 4.6062e-04 - val_loss: 7.7447e-04 - val_mse: 7.7447e-04\n",
      "Epoch 265/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 266/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.3966e-04 - mse: 1.3966e-04 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 267/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 8.0804e-04 - val_mse: 8.0804e-04\n",
      "Epoch 268/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 269/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 270/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 271/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 7.8853e-04 - mse: 7.8853e-04 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 272/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 273/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.8135e-04 - mse: 1.8135e-04 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 274/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.2724e-04 - mse: 2.2724e-04 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 275/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 276/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 6.0843e-04 - mse: 6.0843e-04 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 277/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 278/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 279/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0621 - val_mse: 0.0621\n",
      "Epoch 280/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0735 - mse: 0.0735 - val_loss: 0.4380 - val_mse: 0.4380\n",
      "Epoch 281/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.8217 - mse: 1.8217 - val_loss: 0.6235 - val_mse: 0.6235\n",
      "Epoch 282/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.4653 - mse: 2.4653 - val_loss: 20.3713 - val_mse: 20.3713\n",
      "Epoch 283/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 10.6758 - mse: 10.6758 - val_loss: 134.8400 - val_mse: 134.8400\n",
      "Epoch 284/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 59.6117 - mse: 59.6117 - val_loss: 9.4408 - val_mse: 9.4408\n",
      "Epoch 285/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5.4658 - mse: 5.4658 - val_loss: 2.3862 - val_mse: 2.3862\n",
      "Epoch 286/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.5706 - mse: 0.5706 - val_loss: 0.1388 - val_mse: 0.1388\n",
      "Epoch 287/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 288/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5.5626e-04 - mse: 5.5626e-04 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 289/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 3.7598e-04 - mse: 3.7598e-04 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 290/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.8065e-04 - mse: 4.8065e-04 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 291/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 292/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 6.5439e-04 - mse: 6.5439e-04 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 293/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 9.6703e-04 - mse: 9.6703e-04 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 294/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 295/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 296/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 297/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.7953e-04 - mse: 2.7953e-04 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 298/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.0592e-04 - mse: 4.0592e-04 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 299/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 300/300\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0170 - val_mse: 0.0170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb010376a0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 훈련 \n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit([x1_train, x2_train], y_train, batch_size=1, epochs=300, validation_data=([x1_val, x2_val], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate` 에서도 x의 입력데이터가 2개 이므로 list의 형태를 취해서 들어가야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 655us/step - loss: 0.2494 - mse: 0.2494\n",
      "mse :  [0.24938639998435974, 0.24938639998435974]\n"
     ]
    }
   ],
   "source": [
    "mse = model.evaluate([x1_test, x2_test], y_test, batch_size=1)\n",
    "print('mse : ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict` 에서도 x의 입력데이터가 2개 이므로 list의 형태를 취해서 들어가야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "실제 값 :  [581] 모델 예측 값 :  [581.1665]\n",
      "실제 값 :  [582] 모델 예측 값 :  [582.16876]\n",
      "실제 값 :  [583] 모델 예측 값 :  [583.1712]\n",
      "실제 값 :  [584] 모델 예측 값 :  [584.17365]\n",
      "실제 값 :  [585] 모델 예측 값 :  [585.1761]\n",
      "실제 값 :  [586] 모델 예측 값 :  [586.17847]\n",
      "실제 값 :  [587] 모델 예측 값 :  [587.1809]\n",
      "실제 값 :  [588] 모델 예측 값 :  [588.18335]\n",
      "실제 값 :  [589] 모델 예측 값 :  [589.19604]\n",
      "실제 값 :  [590] 모델 예측 값 :  [590.21606]\n",
      "실제 값 :  [591] 모델 예측 값 :  [591.2345]\n",
      "실제 값 :  [592] 모델 예측 값 :  [592.25305]\n",
      "실제 값 :  [593] 모델 예측 값 :  [593.35034]\n",
      "실제 값 :  [594] 모델 예측 값 :  [594.4546]\n",
      "실제 값 :  [595] 모델 예측 값 :  [595.55865]\n",
      "실제 값 :  [596] 모델 예측 값 :  [596.6626]\n",
      "실제 값 :  [597] 모델 예측 값 :  [597.76654]\n",
      "실제 값 :  [598] 모델 예측 값 :  [598.87054]\n",
      "실제 값 :  [599] 모델 예측 값 :  [599.97455]\n",
      "실제 값 :  [600] 모델 예측 값 :  [601.0785]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict([x1_test, x2_test])\n",
    "\n",
    "print(y_predict.shape)\n",
    "for i in range(len(y_predict)):\n",
    "    print('실제 값 : ', y_test[i], '모델 예측 값 : ', y_predict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Layer\n",
    "concatenate는 단순하게 모델 2개를 엮어주는 역할을 합니다.  \n",
    "인풋이 (10, 3) 인 모델 1과, 모델 2을 엮을 경우 concatenate 시점에서는 (10, 6)으로 연결되게 됨. 합쳐지는 레이어에서 노드들이 단순히 연결하는 역할  \n",
    "아래 나오는 함수들은 더하거나, 빼거나, 평균을 내는 방식으로 합쳐주는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add\n",
    "keras.layers.Add()  \n",
    "layers 끼리 단순한 더하여 합치는 레이어, 동일한 모양의 텐서 목록을 입력으로 사용(더하고자 하는 layers의 노드 수가 같아야 함.)하고 하나의 텐서(입력의 노드수과 같은 shape)를 반환함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 8)            136         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 8)            264         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8)            0           dense_45[0][0]                   \n",
      "                                                                 dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 4)            36          add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 436\n",
      "Trainable params: 436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import Add\n",
    "\n",
    "input1 = Input(shape=(16,))\n",
    "x1 = Dense(8, activation='relu')(input1)\n",
    "\n",
    "input2 = Input(shape=(32,))\n",
    "x2 = Dense(8, activation='relu')(input2)\n",
    "\n",
    "added = Add()([x1, x2])\n",
    "out = Dense(4)(added)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract\n",
    "keras.layers.Subtract()  \n",
    "layers 끼리 단순한 뺄셈하여 합치는 레이어, 동일한 모양의 크기 2인 텐서 목록을 사용하고 동일한 모양의 단일 텐서(입력[0]-입력[1])를 반환함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 8)            136         input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 8)            264         input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 8)            0           dense_56[0][0]                   \n",
      "                                                                 dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 4)            36          subtract_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 436\n",
      "Trainable params: 436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import Add, Subtract\n",
    "\n",
    "input1 = Input(shape=(16,))\n",
    "x1 = Dense(8, activation='relu')(input1)\n",
    "\n",
    "input2 = Input(shape=(32,))\n",
    "x2 = Dense(8, activation='relu')(input2)\n",
    "\n",
    "subtracted = Subtract()([x1, x2])\n",
    "out = Dense(4)(subtracted)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiply\n",
    "keras.layers.Mulitply()  \n",
    "layers 끼리 입력 목록을 요소별로 곱하여 합치는 레이어  \n",
    "* 동일한 shape을 받고(마지막 레이어의 output shape, 노드수가 같아야함.)  \n",
    "* 하나의 텐서를 반환함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average\n",
    "keras.layers.Average()  \n",
    "layers 끼리 입력 목록의 평균으로 합쳐지는 레이어  \n",
    "* 동일한 shape을 받고(마지막 레이어의 output shape, 노드수가 같아야함.)  \n",
    "* 하나의 텐서를 반환함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum\n",
    "keras.layers.Minimum()  \n",
    "layers 끼리 입력 목록의 최소로 계산하는 레이어  \n",
    "* 동일한 shape을 받고(마지막 레이어의 output shape, 노드수가 같아야함.)  \n",
    "* 하나의 텐서를 반환함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate\n",
    "keras.layers.Concatenate(axis=-1)  \n",
    "입력 목록을 연결하는 계층, \n",
    "* 입력 목록을 연결해서\n",
    "* 하나의 텐서로 반환함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot\n",
    "keras.layers.Dot()  \n",
    "두 텐서간 내적을 계산하는 레이어,  \n",
    "* input으로 (batch_size, n)인 a, b에 적용되는 경우 ->\n",
    "* (batch_size, 1) 로 출력함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 함수들과 똑같이 소문자로 함수가 존재함. 내용은 같으나 사용방법만 달라 유의하고 넘어가면 됨.  \n",
    "* Concatenate()([x1, x2])\n",
    "* concatenate([x1, x2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
