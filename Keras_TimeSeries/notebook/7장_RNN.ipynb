{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN(순환신경망,Recurrent Neural Network)은 주로 시계열 분석, 자연어 처리 등 순서가 있는 데이터에 사용하면 좋은 결과가 있는 모델  \n",
    "실제로 주식, 기상, 등을 계산할 때 매우 유효하고 대회에서 시계열 데이터를 다룰 때 많이 사용함.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "가장 기본적인 RNN임.  \n",
    "`x_train` 데이터는 1~5, 2~6, 3~7 로 총 3개의 리스트임.  \n",
    "`y_train` 은 `x_train` 에 대응하는 각각 6, 7, 8 로 이루어진 값이 3개인 벡터로 구성되어있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (3, 5)\n",
      "y_train.shape :  (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 준비\n",
    "import numpy as np \n",
    "x_train = np.array([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "y_train = np.array([[6], [7], [8]])\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 x_train의 컬럼과 y_train 의 벡터의 크기를 맞추기 위해 x_train 을 reshape 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5],\n",
       "       [2, 3, 4, 5, 6],\n",
       "       [3, 4, 5, 6, 7]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [7],\n",
       "       [8]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]],\n",
       "\n",
       "       [[2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]],\n",
       "\n",
       "       [[3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 주의해야할 것이 x의 데이터 구조(3행, 5열, 1 feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (3, 5, 1)\n",
      "y_train.shape :  (3, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y의 벡터는 행의 개수와 대응함. 그래서 x의 행의 수 = y의 벡터의 크기와 같음. (???)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 7)                 63        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 100\n",
      "Trainable params: 100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. 모델구성\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(7, input_shape=(5, 1), activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 2ms/step - loss: 80.6390 - mse: 80.6390\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 77.1209 - mse: 77.1209\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 73.7838 - mse: 73.7838\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 61.2677 - mse: 61.2677\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 65.6039 - mse: 65.6039\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 52.5310 - mse: 52.5310\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 53.8169 - mse: 53.8169\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 51.4989 - mse: 51.4989\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 46.1325 - mse: 46.1325\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 44.1121 - mse: 44.1121\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 51.1993 - mse: 51.1993\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.6936 - mse: 41.6936\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 43.4009 - mse: 43.4009\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.2282 - mse: 41.2282\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.6817 - mse: 41.6817\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 34.8567 - mse: 34.8567\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 35.0375 - mse: 35.0375\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 29.4037 - mse: 29.4037\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 33.0972 - mse: 33.0972\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 31.0086 - mse: 31.0086\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 27.3047 - mse: 27.3047\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.8638 - mse: 22.8638\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.0070 - mse: 22.0070\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 23.1841 - mse: 23.1841\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 18.2799 - mse: 18.2799\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 17.6171 - mse: 17.6171\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 17.7238 - mse: 17.7238\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 14.5610 - mse: 14.5610\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 13.6586 - mse: 13.6586\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 11.3345 - mse: 11.3345\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 10.1032 - mse: 10.1032\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.7468 - mse: 9.7468\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.0989 - mse: 8.0989\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.9010 - mse: 6.9010\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.8709 - mse: 5.8709\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.9566 - mse: 4.9566\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.2461 - mse: 4.2461\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.3823 - mse: 3.3823\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7445 - mse: 2.7445\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1897 - mse: 2.1897\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6857 - mse: 1.6857\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5237 - mse: 1.5237\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.0278\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7763 - mse: 0.7763\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4626 - mse: 0.4626\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5564 - mse: 0.5564\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2277 - mse: 0.2277\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2230 - mse: 0.2230\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2716 - mse: 0.2716\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1103 - mse: 0.1103    \n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0568 - mse: 0.0568\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0518 - mse: 0.0518    \n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0933 - mse: 0.0933\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1186 - mse: 0.1186\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1109 - mse: 0.1109\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0579 - mse: 0.0579\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1135 - mse: 0.1135\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0985 - mse: 0.0985\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0612 - mse: 0.0612\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0935 - mse: 0.0935\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0720 - mse: 0.0720\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0562 - mse: 0.0562\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0628 - mse: 0.0628\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0945 - mse: 0.0945\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0923 - mse: 0.0923\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0381 - mse: 0.0381\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0486 - mse: 0.0486\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0821 - mse: 0.0821\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0342 - mse: 0.0342\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0418 - mse: 0.0418\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0397 - mse: 0.0397\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0433 - mse: 0.0433    \n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0775 - mse: 0.0775\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0290 - mse: 0.0290    \n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0498 - mse: 0.0498\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0338 - mse: 0.0338\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0692 - mse: 0.0692\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0259 - mse: 0.0259\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0251 - mse: 0.0251\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0602 - mse: 0.0602\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0330 - mse: 0.0330\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0319 - mse: 0.0319\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0297 - mse: 0.0297\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0367 - mse: 0.0367\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0352 - mse: 0.0352\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0268 - mse: 0.0268    \n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0185 - mse: 0.0185    \n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0463 - mse: 0.0463\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0406 - mse: 0.0406\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0389 - mse: 0.0389\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214    \n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206    \n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0348 - mse: 0.0348\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0352 - mse: 0.0352\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0184 - mse: 0.0184    \n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0328 - mse: 0.0328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc96e1f8ee0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "x_predict reshape :  (1, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "# 4. 예측\n",
    "x_predict = np.array([[4, 5, 6, 7, 8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0], x_predict.shape[1], -1)\n",
    "print(\"x_predict reshape : \", x_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 :  [[9.281104]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(x_predict)\n",
    "print(\"예측값 : \", y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "RNN 에서 가장 많이 쓰는 모델.    \n",
    "원리는 동일하지만 Simple RNN 보다 파라미터 수가 월등히 많아지고 성능이 좋음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (3, 5)\n",
      "y_train.shape :  (3, 1)\n",
      "x_train.shape :  (3, 5, 1)\n",
      "y_train.shape :  (3, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 7)                 252       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 준비\n",
    "import numpy as np \n",
    "x_train = np.array([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "y_train = np.array([[6], [7], [8]])\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)\n",
    "\n",
    "# reshape\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)\n",
    "\n",
    "# 2. 모델구성\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(7, input_shape=(5, 1), activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpleRNN의 경우 첫번째 레이어에서 param 수가 63, LSTM의 경우 첫번째 레이어에서 param 수가 252  \n",
    "대회에 참가하는 경우 시간이 부족할때가 많음. 그럴때는 simpleRNN, DNN 으로 돌려서 우선적으로 acc 구한 뒤 LSTM으로 모델을 훈련시키는 것도 한가지 방법이 될 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 3ms/step - loss: 36.9286 - mse: 36.9286\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 36.1762 - mse: 36.1762\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 29.6824 - mse: 29.6824\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 31.0556 - mse: 31.0556\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 32.4573 - mse: 32.4573\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 27.6147 - mse: 27.6147\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 27.9869 - mse: 27.9869\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 32.0819 - mse: 32.0819\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 26.3807 - mse: 26.3807\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 25.5379 - mse: 25.5379\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 24.6497 - mse: 24.6497\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 26.0464 - mse: 26.0464\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 24.9353 - mse: 24.9353\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 23.7733 - mse: 23.7733\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 23.5460 - mse: 23.5460\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.8425 - mse: 22.8425\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.4205 - mse: 21.4205\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 17.0009 - mse: 17.0009\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 17.1706 - mse: 17.1706\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 14.7053 - mse: 14.7053\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 13.0443 - mse: 13.0443\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 11.2127 - mse: 11.2127\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 11.0145 - mse: 11.0145\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.7698 - mse: 8.7698\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.7648 - mse: 7.7648\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.9176 - mse: 5.9176\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.6733 - mse: 4.6733\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.3196 - mse: 3.3196\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1974 - mse: 2.1974\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4297 - mse: 1.4297\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5184 - mse: 0.5184\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3447 - mse: 0.3447\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1997 - mse: 0.1997\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4413 - mse: 0.4413\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6341 - mse: 0.6341\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2967 - mse: 0.2967\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4308 - mse: 0.4308\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2706 - mse: 0.2706\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3983 - mse: 0.3983\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1721 - mse: 0.1721    \n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1464 - mse: 0.1464\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3390 - mse: 0.3390\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3513 - mse: 0.3513\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2215 - mse: 0.2215\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1495 - mse: 0.1495\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2109 - mse: 0.2109\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2021 - mse: 0.2021\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3200 - mse: 0.3200\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2945 - mse: 0.2945\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1284 - mse: 0.1284\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1272 - mse: 0.1272\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1583 - mse: 0.1583\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2215 - mse: 0.2215\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2139 - mse: 0.2139\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1557 - mse: 0.1557\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1161 - mse: 0.1161\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1961 - mse: 0.1961\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1491 - mse: 0.1491\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2593 - mse: 0.2593\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1919 - mse: 0.1919\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2372 - mse: 0.2372\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2291 - mse: 0.2291\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2403 - mse: 0.2403\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2212 - mse: 0.2212\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1011 - mse: 0.1011\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1678 - mse: 0.1678\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1167 - mse: 0.1167\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2209 - mse: 0.2209\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1576 - mse: 0.1576\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2191 - mse: 0.2191\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1131 - mse: 0.1131\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1500 - mse: 0.1500\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1971 - mse: 0.1971\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1099 - mse: 0.1099\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1070 - mse: 0.1070\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1048 - mse: 0.1048\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1885 - mse: 0.1885\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1733 - mse: 0.1733\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1002 - mse: 0.1002\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1363 - mse: 0.1363\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1006 - mse: 0.1006\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0723 - mse: 0.0723\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1705 - mse: 0.1705\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0949 - mse: 0.0949\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0924 - mse: 0.0924\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1553 - mse: 0.1553\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1204 - mse: 0.1204\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0649 - mse: 0.0649\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1513 - mse: 0.1513\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1541 - mse: 0.1541\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0805 - mse: 0.0805\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0752 - mse: 0.0752\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0822 - mse: 0.0822\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1022 - mse: 0.1022\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1001 - mse: 0.1001\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1326 - mse: 0.1326\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1256 - mse: 0.1256\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1302 - mse: 0.1302\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0743 - mse: 0.0743\n",
      "(1, 5)\n",
      "x_predict reshape :  (1, 5, 1)\n",
      "예측값 :  [[9.268801]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# 4. 예측\n",
    "x_predict = np.array([[4, 5, 6, 7, 8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0], x_predict.shape[1], -1)\n",
    "print(\"x_predict reshape : \", x_predict.shape)\n",
    "\n",
    "y_predict = model.predict(x_predict)\n",
    "print(\"예측값 : \", y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "뉴욕대학교의 조경현 교수님이 만든 모델임. LSTM을 보완하였는데 사실 GRU 는 LSTM의 아웃풋 게이트를 두지 않고, LSTM을 간단하게 변경한 구조  \n",
    "약간의 축소 버전이라고 생각. 속도는 LSTM 보다 빨라지고, 성능은 거의 동일하거나, 약간 낮은 것을로 알려져 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (3, 5)\n",
      "y_train.shape :  (3, 1)\n",
      "x_train.shape :  (3, 5, 1)\n",
      "y_train.shape :  (3, 1)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 7)                 210       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 247\n",
      "Trainable params: 247\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 준비\n",
    "import numpy as np \n",
    "x_train = np.array([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "y_train = np.array([[6], [7], [8]])\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)\n",
    "\n",
    "# reshape\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)\n",
    "\n",
    "# 2. 모델구성\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(7, input_shape=(5, 1), activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 3ms/step - loss: 51.1644 - mse: 51.1644\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 51.0012 - mse: 51.0012\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 45.9784 - mse: 45.9784\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 47.1845 - mse: 47.1845\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 47.0161 - mse: 47.0161\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 45.4844 - mse: 45.4844\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 52.2271 - mse: 52.2271\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 46.4774 - mse: 46.4774\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 51.8252 - mse: 51.8252\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 46.0874 - mse: 46.0874\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 51.3936 - mse: 51.3936\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 49.0788 - mse: 49.0788\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 42.2953 - mse: 42.2953\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 52.2495 - mse: 52.2495\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 48.3526 - mse: 48.3526\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 51.7093 - mse: 51.7093\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.3642 - mse: 41.3642\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.1112 - mse: 41.1112\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 49.2847 - mse: 49.2847\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 48.9667 - mse: 48.9667\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 42.0492 - mse: 42.0492\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.7422 - mse: 41.7422\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 42.6783 - mse: 42.6783\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 47.5664 - mse: 47.5664\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 39.0090 - mse: 39.0090\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 40.3819 - mse: 40.3819\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 47.8165 - mse: 47.8165\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 43.9655 - mse: 43.9655\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 39.2081 - mse: 39.2081\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 37.1243 - mse: 37.1243\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 42.6022 - mse: 42.6022\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 43.9952 - mse: 43.9952\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 44.8498 - mse: 44.8498\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.0668 - mse: 41.0668\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 40.5227 - mse: 40.5227\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 34.3773 - mse: 34.3773\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 41.1831 - mse: 41.1831\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 35.9573 - mse: 35.9573\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 32.8080 - mse: 32.8080\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 40.5380 - mse: 40.5380\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 38.5845 - mse: 38.5845\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 39.1082 - mse: 39.1082\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 32.8901 - mse: 32.8901\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 37.6155 - mse: 37.6155\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 35.6702 - mse: 35.6702\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 34.8958 - mse: 34.8958\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 34.1030 - mse: 34.1030\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 29.4120 - mse: 29.4120\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 28.6768 - mse: 28.6768\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 31.6339 - mse: 31.6339\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 27.1431 - mse: 27.1431\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 28.5197 - mse: 28.5197\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 28.9848 - mse: 28.9848\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 24.7273 - mse: 24.7273\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 23.9038 - mse: 23.9038\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 24.9851 - mse: 24.9851\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 25.2429 - mse: 25.2429\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 20.6503 - mse: 20.6503\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 20.4597 - mse: 20.4597\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 18.9496 - mse: 18.9496\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 17.2185 - mse: 17.2185\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 20.3020 - mse: 20.3020\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 16.3267 - mse: 16.3267\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 14.6948 - mse: 14.6948\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 13.8578 - mse: 13.8578\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 16.7791 - mse: 16.7791\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 15.6980 - mse: 15.6980\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 13.4193 - mse: 13.4193\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 13.5846 - mse: 13.5846\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 12.1164 - mse: 12.1164\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 11.1390 - mse: 11.1390\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 10.1830 - mse: 10.1830\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.7491 - mse: 7.7491\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.9950 - mse: 6.9950\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 5.9398 - mse: 5.9398\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.7897 - mse: 5.7897\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.6550 - mse: 4.6550\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.0678 - mse: 4.0678\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.5195 - mse: 3.5195\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.1862 - mse: 3.1862\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5458 - mse: 2.5458\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8427 - mse: 2.8427\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8486 - mse: 1.8486\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8362 - mse: 1.8362\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2537 - mse: 1.2537\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9437 - mse: 0.9437\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7291 - mse: 0.7291\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5805 - mse: 0.5805\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3831 - mse: 0.3831\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3489 - mse: 0.3489\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2763 - mse: 0.2763\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1672 - mse: 0.1672\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0571 - mse: 0.0571\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0272 - mse: 0.0272\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.8553e-04 - mse: 4.8553e-04\n",
      "(1, 5)\n",
      "x_predict reshape :  (1, 5, 1)\n",
      "예측값 :  [[8.963178]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# 4. 예측\n",
    "x_predict = np.array([[4, 5, 6, 7, 8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0], x_predict.shape[1], -1)\n",
    "print(\"x_predict reshape : \", x_predict.shape)\n",
    "\n",
    "y_predict = model.predict(x_predict)\n",
    "print(\"예측값 : \", y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM\n",
    "RNN 에서는 우리의 실행의 방향이 한 방향이었고, 시간의 순서대로 쭉 진행되었음.  \n",
    "예를 들어 1, 2, 3, 4, 5라는 데이터에서 1부터, 5까지 순차적으로 RNN 하는 방식이었음.  \n",
    "하지만 이를 역순으로 5, 4, 3, 2, 1 이라는 데이터 역시 순차적 데이터가 될 수 있음  \n",
    "여기서 착안한 것이 Bidirectional 임. 우선 RNN을 순방향으로 진행한 후 역방향으로 다시 훈련시키는 방법 그래서 이름도 양방향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (3, 5)\n",
      "y_train.shape :  (3, 1)\n",
      "x_train.shape :  (3, 5, 1)\n",
      "y_train.shape :  (3, 1)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 14)                504       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 60        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 569\n",
      "Trainable params: 569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 준비\n",
    "import numpy as np \n",
    "x_train = np.array([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "y_train = np.array([[6], [7], [8]])\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)\n",
    "\n",
    "# reshape\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('y_train.shape : ', y_train.shape)\n",
    "\n",
    "# 2. 모델구성\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Bidirectional\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(7, activation='relu'), input_shape=(5, 1)))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 레이어의 param의 수가 LSTM은 252개 였던 것에 비하면 2배가 증가했음.  \n",
    "Bidirectional LSTM을 사용하게 되면 LSTM의 2배의 파라미터를 잡기때문에 속도 또한 일반 LSTM 보다 느려짐.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 2ms/step - loss: 0.2444 - mse: 0.2444\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2718 - mse: 0.2718\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0992 - mse: 0.0992\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1651 - mse: 0.1651\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1492 - mse: 0.1492\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1329 - mse: 0.1329\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0892 - mse: 0.0892\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0970 - mse: 0.0970\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1012 - mse: 0.1012\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0708 - mse: 0.0708\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1128 - mse: 0.1128\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1000 - mse: 0.1000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0826 - mse: 0.0826\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0434 - mse: 0.0434    \n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0707 - mse: 0.0707\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0646 - mse: 0.0646\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0241 - mse: 0.0241    \n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0604 - mse: 0.0604\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0256 - mse: 0.0256\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0228 - mse: 0.0228\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117    \n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103    \n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0133 - mse: 0.0133\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 0.0124\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0116 - mse: 0.0116\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0034 - mse: 0.0034    \n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0026 - mse: 0.0026    \n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0018 - mse: 0.0018    \n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0010 - mse: 0.0010    \n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0014 - mse: 0.0014    \n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 0.0012    \n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0013 - mse: 0.0013    \n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0013 - mse: 0.0013    \n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0011    \n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 8.7073e-04 - mse: 8.7073e-04\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.8027e-04 - mse: 7.8027e-04\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 8.7283e-04 - mse: 8.7283e-04\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.0542e-04 - mse: 9.0542e-04\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 0.0012    \n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0011    \n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.9378e-04 - mse: 7.9378e-04\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.8592e-04 - mse: 6.8592e-04\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 7.5935e-04 - mse: 7.5935e-04\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.1971e-04 - mse: 9.1971e-04\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0011    \n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.5457e-04 - mse: 9.5457e-04\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 9.1749e-04 - mse: 9.1749e-04\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0010 - mse: 0.0010    \n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0010 - mse: 0.0010    \n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.3222e-04 - mse: 8.3222e-04\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 8.1222e-04 - mse: 8.1222e-04\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.6717e-04 - mse: 9.6717e-04\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.2783e-04 - mse: 9.2783e-04\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.1336e-04 - mse: 7.1336e-04\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 5.2617e-04 - mse: 5.2617e-04\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 5.3682e-04 - mse: 5.3682e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.6259e-04 - mse: 8.6259e-04\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 9.7960e-04 - mse: 9.7960e-04\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.2819e-04 - mse: 7.2819e-04\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 7.7029e-04 - mse: 7.7029e-04\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.1243e-04 - mse: 5.1243e-04\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 8.8827e-04 - mse: 8.8827e-04\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.3508e-04 - mse: 6.3508e-04\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.4484e-04 - mse: 7.4484e-04\n",
      "(1, 5)\n",
      "x_predict reshape :  (1, 5, 1)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc91a59a820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "예측값 :  [[8.808506]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# 4. 예측\n",
    "x_predict = np.array([[4, 5, 6, 7, 8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0], x_predict.shape[1], -1)\n",
    "print(\"x_predict reshape : \", x_predict.shape)\n",
    "\n",
    "y_predict = model.predict(x_predict)\n",
    "print(\"예측값 : \", y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 레이어 - LSTM 레이어 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재까지는 LSTM 레이어 뒤에 Dense 층을 연결했었음.  \n",
    "LSTM 레이어도 2개 이상 연결이 가능함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_4 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-bf58bf3be81b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2682\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2684\u001b[0;31m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[1;32m   2685\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[1;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[1;32m    220\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_4 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 7)"
     ]
    }
   ],
   "source": [
    "# 2. 모델구성\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(7, input_shape=(5, 1), activation='relu'))\n",
    "model.add(LSTM(8))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행결과 valueError가 발생함, 이전 LSTM 레이어의 아웃풋이 두번째 LSTM 레이어의 입력차원이랑 맞지않다는 내용  \n",
    "LSTM은 (sample, time steps, feature), 3개의 차원을 필요로 함. \n",
    "LSTM의 output의 차원은 (None, 7) 형식으로 2차원으로 나오게 됨. \n",
    "\n",
    "케라스에서는 LSTM 레이어의 차원을 맞춰주기 위해 return_sequence라는 파라미터를 지원함. 한 마디로 이전 차원을 그대로 유지해주겠다는 의미  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 5, 7)              252       \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 8)                 512       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 805\n",
      "Trainable params: 805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. 모델구성\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(7, input_shape=(5, 1), activation='relu', return_sequences=True))\n",
    "model.add(LSTM(8))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input shape(3, 5, 1) -> LSTM 1 (None, 5, 7) -> LSTM (None, 8)  \n",
    "LSTM 1 에서 return_sequences 를 True 값을 줬기 때문에 output shape 가 feature 값만 레이어의 노드수에 맞게 변하고 차원은 그대로임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 4ms/step - loss: 49.2544 - mse: 49.2544\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 39.5701 - mse: 39.5701\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 45.7112 - mse: 45.7112\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 42.9460 - mse: 42.9460\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 43.4310 - mse: 43.4310\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 43.4404 - mse: 43.4404\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 37.0273 - mse: 37.0273\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 39.8648 - mse: 39.8648\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 39.6930 - mse: 39.6930\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 33.6010 - mse: 33.6010\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 34.6360 - mse: 34.6360\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 31.2216 - mse: 31.2216\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 29.3698 - mse: 29.3698\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 27.0400 - mse: 27.0400\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 31.0204 - mse: 31.0204\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 28.2615 - mse: 28.2615\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 28.4237 - mse: 28.4237\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 22.3643 - mse: 22.3643\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 26.5787 - mse: 26.5787\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.0884 - mse: 21.0884\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 20.2104 - mse: 20.2104\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 19.0194 - mse: 19.0194\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 17.8442 - mse: 17.8442\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 18.2469 - mse: 18.2469\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 15.3738 - mse: 15.3738\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 15.8989 - mse: 15.8989\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 16.0694 - mse: 16.0694\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 12.1795 - mse: 12.1795\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 11.1771 - mse: 11.1771\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 12.7451 - mse: 12.7451\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 12.1024 - mse: 12.1024\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 8.3586 - mse: 8.3586\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 7.5326 - mse: 7.5326\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 6.7138 - mse: 6.7138\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 5.9419 - mse: 5.9419\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 7.3720 - mse: 7.3720\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 5.3799 - mse: 5.3799\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 3.4076 - mse: 3.4076\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.8995 - mse: 2.8995\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 4.5010 - mse: 4.5010\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 2.4626 - mse: 2.4626\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.0776 - mse: 2.0776\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 2.7997 - mse: 2.7997\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.3625 - mse: 1.3625\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.2145 - mse: 1.2145\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7277 - mse: 0.7277\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.5500 - mse: 1.5500\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7234 - mse: 0.7234\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4958 - mse: 0.4958\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.0535 - mse: 1.0535\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3475 - mse: 0.3475\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2849 - mse: 0.2849\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4220 - mse: 0.4220\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6716 - mse: 0.6716\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2675 - mse: 0.2675\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2247 - mse: 0.2247\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5544 - mse: 0.5544\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2845 - mse: 0.2845\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3860 - mse: 0.3860\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4512 - mse: 0.4512\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2164 - mse: 0.2164    \n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4200 - mse: 0.4200\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2479 - mse: 0.2479\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2443 - mse: 0.2443\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3889 - mse: 0.3889\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3885 - mse: 0.3885\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2147 - mse: 0.2147\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3762 - mse: 0.3762\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3090 - mse: 0.3090\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4164 - mse: 0.4164\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3003 - mse: 0.3003\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3646 - mse: 0.3646\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3602 - mse: 0.3602\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2218 - mse: 0.2218\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3517 - mse: 0.3517\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3941 - mse: 0.3941\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3472 - mse: 0.3472\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2132 - mse: 0.2132\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3809 - mse: 0.3809\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3320 - mse: 0.3320\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1830 - mse: 0.1830\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2569 - mse: 0.2569\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3349 - mse: 0.3349\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1757 - mse: 0.1757\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3274 - mse: 0.3274\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3571 - mse: 0.3571\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3022 - mse: 0.3022\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1933 - mse: 0.1933\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1911 - mse: 0.1911\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1890 - mse: 0.1890\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2282 - mse: 0.2282\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2217 - mse: 0.2217\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2162 - mse: 0.2162\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3013 - mse: 0.3013\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2115 - mse: 0.2115\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3212 - mse: 0.3212\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1474 - mse: 0.1474\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1449 - mse: 0.1449\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1426 - mse: 0.1426\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2481 - mse: 0.2481\n",
      "(1, 5)\n",
      "x_predict reshape :  (1, 5, 1)\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc91a5ffee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "예측값 :  [[7.4454107]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# 4. 예측\n",
    "x_predict = np.array([[4, 5, 6, 7, 8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0], x_predict.shape[1], -1)\n",
    "print(\"x_predict reshape : \", x_predict.shape)\n",
    "\n",
    "y_predict = model.predict(x_predict)\n",
    "print(\"예측값 : \", y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
